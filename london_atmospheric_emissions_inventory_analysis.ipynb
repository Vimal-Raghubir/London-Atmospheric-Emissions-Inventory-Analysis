{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# London Atmospheric Emissions Inventory Analysis\n",
    "\n",
    "The code below is used to predict atmospheric emissions given data provided for London's atmospheric emissions in several years. These years are 2008, 2010, 2013, and 2020. The dataset focuses on major road in London and the atmospheric emissions recorded yearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in datasets and concatenate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2008_org = pd.read_excel('Major Roads/LAEI2013_MajorRoads_EmissionsbyLink_2008.xlsx')\n",
    "df_2010_org = pd.read_excel('Major Roads/LAEI2013_MajorRoads_EmissionsbyLink_2010.xlsx')\n",
    "df_2013_org = pd.read_excel('Major Roads/LAEI2013_MajorRoads_EmissionsbyLink_2013.xlsx')\n",
    "df_2020_org = pd.read_excel('Major Roads/LAEI2013_MajorRoads_EmissionsbyLink_2020.xlsx')\n",
    "\n",
    "# Remove unneeded column\n",
    "df_2020_org = df_2020_org.drop(['DotRef'], axis=1)\n",
    "\n",
    "# Renaming Borough_ExactCut to the same name so they combine with each other after concatenation\n",
    "df_2008_org = df_2008_org.rename(columns={'BoroughExactCut': 'Borough_ExactCut'})\n",
    "df_2013_org = df_2013_org.rename(columns={\"BoroughExactCut\": \"Borough_ExactCut\"})\n",
    "df_2020_org = df_2020_org.rename(columns={\"BoroughExactCut\": \"Borough_ExactCut\"})\n",
    "\n",
    "\n",
    "main_df = pd.concat([df_2008_org, df_2010_org, df_2013_org, df_2020_org])\n",
    "main_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert all object columns to Numeric Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_object_columns_to_numeric(df):\n",
    "    \"\"\"\n",
    "        This function takes a dataframe and will first find all object type columns, loop through each of them and encode them using a LabelEncoder object. This will ensure\n",
    "        that the dataframe will not contain anymore object columns.\n",
    "    \"\"\"\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns # Code to find all column names that are object type\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        df_encoded[col] = label_encoder.fit_transform(df_encoded[col]) # Label encode each object type column\n",
    "        df_encoded[col] = df_encoded[col].astype(int) # And convert the column to integer\n",
    "\n",
    "    df_encoded.info()\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to split the data based on target column and filter train/test set based on the Year of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_based_on_target_label(df, target_column, test_year):\n",
    "    \"\"\"\n",
    "        This function is responsible for taking in a dataframe, a target column, and a test year and will split the dataframe into an X and Y dataframe based on the target column.\n",
    "        Then will take all rows with a Year value of less than the test_year and will use this as the training set and everything equal to or above the test year is part of the test set.\n",
    "        E.g. if test_year = 2020 then all rows < 2020 will be training and all >= 2020 will be part of the test set.\n",
    "    \"\"\"\n",
    "    train_set = df[df['Year'] < test_year]\n",
    "    test_set = df[df['Year'] >= test_year]\n",
    "\n",
    "    X_train = train_set.drop(target_column, axis=1)\n",
    "    y_train = train_set[target_column]\n",
    "\n",
    "    X_test = test_set.drop(target_column, axis=1)\n",
    "    y_test = test_set[target_column]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to analyze the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(y_test, predictions, model_type):\n",
    "    \"\"\"\n",
    "        This function is used to compare the predictions of the model and the actual ground truth labels to assess performance\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "    # R-squared (R2) Score\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(\"R-squared (R2) Score:\", r2)\n",
    "\n",
    "    # Plotting actual vs predicted values\n",
    "    plt.scatter(y_test, predictions)\n",
    "    plt.xlabel('Actual values')\n",
    "    plt.ylabel('Predicted values')\n",
    "    plt.title(model_type + ' Actual vs. Predicted Values')\n",
    "    # Plotting the identity line; perfect predictions would lie on this line\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    plt.show()\n",
    "\n",
    "    return mse, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_analyze_model(X_train, y_train, X_test, y_test, model, model_type):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    # Call analyze function to analyze the performance of the model\n",
    "    mse, rmse = analyze_model(y_test, predictions, model_type)\n",
    "    return mse, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to build the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_and_analyze_model(X_train, y_train, X_test, y_test, model_type):\n",
    "    \"\"\"\n",
    "        This function will be used to take the data and scale it, convert to tensors, and train a Neural Network model and generate predictions from it. Then will call the analyze function to analyze performance.\n",
    "    \"\"\"\n",
    "    # Standardize features by removing the mean and scaling to unit variance\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Reshape y tensors to match the output shape\n",
    "    y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "    y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "    # Initialize the network\n",
    "    input_size = X_train_tensor.shape[1]\n",
    "    model = Net(input_size)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training the model\n",
    "    epochs = 8000\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Predict on the test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "\n",
    "    # Compute the loss (MSE) and other metrics (MAE and RÂ²)\n",
    "    mse = criterion(predictions, y_test_tensor).item()\n",
    "    mae = torch.mean(torch.abs(predictions - y_test_tensor)).item()\n",
    "    r2 = 1 - (torch.sum((y_test_tensor - predictions) ** 2) / torch.sum((y_test_tensor - torch.mean(y_test_tensor)) ** 2)).item()\n",
    "        \n",
    "    print(f\"Local mse, mae, and r2 values are {mse}, {mae}, {r2}\")\n",
    "    mse, rmse = analyze_model(y_test_tensor, predictions, model_type)\n",
    "    return mse, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_and_train_models(df, target_column, model_list, test_year=2020):\n",
    "    df_encoded = convert_object_columns_to_numeric(df)\n",
    "    X_train, X_test, y_train, y_test = split_data_based_on_target_label(df_encoded, target_column, test_year)\n",
    "    lowest_mse_algorithm = \"\"\n",
    "    lowest_mse = 999999\n",
    "    lowest_rmse_algorithm = \"\"\n",
    "    lowest_rmse = 999999\n",
    "    for model in model_list:\n",
    "        if model != \"NeuralNetwork\":\n",
    "            mse, rmse = train_and_analyze_model(X_train, y_train, X_test, y_test, model_list[model], model)\n",
    "        else:\n",
    "            mse, rmse = build_neural_network_and_analyze_model(X_train, y_train, X_test, y_test, model)\n",
    "        if mse < lowest_mse:\n",
    "            lowest_mse = mse\n",
    "            lowest_mse_algorithm = model\n",
    "    \n",
    "        if rmse < lowest_rmse:\n",
    "            lowest_rmse = rmse\n",
    "            lowest_rmse_algorithm = model\n",
    "\n",
    "    print(f\"Best performing algorithm according to Mean Squared Error is {lowest_mse_algorithm} and has a value of {lowest_mse}\")\n",
    "    print(f\"Best performing algorithm according to Root Mean Squared Error is {lowest_rmse_algorithm} and has a value of {lowest_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = {\"LinearRegression\": LinearRegression(), \"DecisionTreeRegressor\": DecisionTreeRegressor(), \"NeuralNetwork\": True}\n",
    "\n",
    "\n",
    "split_dataset_and_train_models(main_df, \"PetrolCar\", model_list, 2020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
